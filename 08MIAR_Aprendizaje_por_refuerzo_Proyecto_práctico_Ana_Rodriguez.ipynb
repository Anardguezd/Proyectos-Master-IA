{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUehXgCyIRdq"
   },
   "source": [
    "# Actividad - Proyecto práctico\n",
    "\n",
    "\n",
    "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
    "*   Alumno 1: García Giadás, Ana\n",
    "*   Alumno 2: Rodriguez Dominguez, Ana\n",
    "*   Alumno 3: Slonchenko Koberidze, Anton\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwpYlnjWJhS9"
   },
   "source": [
    "---\n",
    "## **PARTE 1** - Instalación y requisitos previos\n",
    "\n",
    "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
    "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
    "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
    "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
    "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU2BPrK2JkP0"
   },
   "source": [
    "---\n",
    "### 1.1. Preparar enviroment (solo local)\n",
    "\n",
    "\n",
    "\n",
    "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
    "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
    "2. Instalar Anaconda\n",
    "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
    "\n",
    "\n",
    "```\n",
    "conda create --name miar_rl python=3.8\n",
    "conda activate miar_rl\n",
    "cd \"PATH_TO_FOLDER\"\n",
    "conda install git\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "4. Abrir la notebook con *jupyter-notebook*.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "jupyter-notebook\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-kixNPiJqTc"
   },
   "source": [
    "---\n",
    "### 1.2. Localizar entorno de trabajo: Google colab o local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_YDFwZ-JscI"
   },
   "outputs": [],
   "source": [
    "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
    "mount='/content/gdrive'\n",
    "drive_root = mount + \"/My Drive/0- Master VIU/Reinforcement Learning-Proyecto\"\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Dp_a1iBJ0tf"
   },
   "source": [
    "---\n",
    "### 1.3. Montar carpeta de datos local (solo Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24515,
     "status": "ok",
     "timestamp": 1687295920397,
     "user": {
      "displayName": "Anton S",
      "userId": "11207087415458905114"
     },
     "user_tz": -120
    },
    "id": "I6n7MIefJ21i",
    "outputId": "8bcb7231-47a9-4da1-f3f1-0653e0bc5ead"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're running Colab\n",
      "Colab: mounting Google drive on  /content/gdrive\n",
      "Mounted at /content/gdrive\n",
      "\n",
      "Colab: making sure  /content/gdrive/My Drive/0- Master VIU/Reinforcement Learning-Proyecto  exists.\n",
      "\n",
      "Colab: Changing directory to  /content/gdrive/My Drive/0- Master VIU/Reinforcement Learning-Proyecto\n",
      "/content/gdrive/My Drive/0- Master VIU/Reinforcement Learning-Proyecto\n",
      "Archivos en el directorio: \n",
      "['Anton', 'Ana R', 'Ana G', 'More', 'Borrador.ipynb', 'Proyecto_práctico.ipynb']\n"
     ]
    }
   ],
   "source": [
    "# Switch to the directory on the Google Drive that you want to use\n",
    "import os\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    # Mount the Google Drive at mount\n",
    "    print(\"Colab: mounting Google drive on \", mount)\n",
    "\n",
    "    drive.mount(mount)\n",
    "\n",
    "    # Create drive_root if it doesn't exist\n",
    "    create_drive_root = True\n",
    "    if create_drive_root:\n",
    "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
    "      os.makedirs(drive_root, exist_ok=True)\n",
    "\n",
    "    # Change to the directory\n",
    "    print(\"\\nColab: Changing directory to \", drive_root)\n",
    "    %cd $drive_root\n",
    "# Verify we're in the correct working directory\n",
    "%pwd\n",
    "print(\"Archivos en el directorio: \")\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1ZSL5bpJ560"
   },
   "source": [
    "---\n",
    "### 1.4. Instalar librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 240249,
     "status": "ok",
     "timestamp": 1687296173303,
     "user": {
      "displayName": "Anton S",
      "userId": "11207087415458905114"
     },
     "user_tz": -120
    },
    "id": "UbVRjvHCJ8UF",
    "outputId": "197c7060-d769-4347-cc3f-b81f653ec858"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting gym==0.17.3\n",
      "  Downloading gym-0.17.3.tar.gz (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.22.4)\n",
      "Collecting pyglet<=1.5.0,>=1.4.0 (from gym==0.17.3)\n",
      "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0 (from gym==0.17.3)\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (0.18.3)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654622 sha256=547fa759540f7e1d0adca35e47b665658956bf3b7339516cea7ff59c53d96c41\n",
      "  Stored in directory: /root/.cache/pip/wheels/af/4b/74/fcfc8238472c34d7f96508a63c962ff3ac9485a9a4137afd4e\n",
      "Successfully built gym\n",
      "Installing collected packages: pyglet, cloudpickle, gym\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.2.1\n",
      "    Uninstalling cloudpickle-2.2.1:\n",
      "      Successfully uninstalled cloudpickle-2.2.1\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.25.2\n",
      "    Uninstalling gym-0.25.2:\n",
      "      Successfully uninstalled gym-0.25.2\n",
      "Successfully installed cloudpickle-1.6.0 gym-0.17.3 pyglet-1.5.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting git+https://github.com/Kojoley/atari-py.git\n",
      "  Cloning https://github.com/Kojoley/atari-py.git to /tmp/pip-req-build-c7pds6zw\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-c7pds6zw\n",
      "  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from atari-py==1.2.2) (1.22.4)\n",
      "Building wheels for collected packages: atari-py\n",
      "  Building wheel for atari-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for atari-py: filename=atari_py-1.2.2-cp310-cp310-linux_x86_64.whl size=4812112 sha256=2d55aab82019517cd19e15bca90f49918b6bdc952d006dd8edab3418fa73da74\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5sm4tchb/wheels/6c/9b/f1/8a80a63a233d6f4eb2e2ee8c7357f4875f21c3ffc7f2613f9f\n",
      "Successfully built atari-py\n",
      "Installing collected packages: atari-py\n",
      "Successfully installed atari-py-1.2.2\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting keras-rl2==1.0.5\n",
      "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-rl2==1.0.5) (2.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.3.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.54.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.8.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.4.10)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (16.0.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.22.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.5.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.32.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2==1.0.5) (0.40.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2==1.0.5) (0.1.0)\n",
      "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2==1.0.5) (1.10.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.7.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.2.2)\n",
      "Installing collected packages: keras-rl2\n",
      "Successfully installed keras-rl2-1.0.5\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting tensorflow==2.8\n",
      "  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (23.3.3)\n",
      "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.8.0)\n",
      "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8)\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (16.0.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.22.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.14.1)\n",
      "Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8)\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8)\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8)\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.32.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.54.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.40.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.17.3)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.27.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.2)\n",
      "Installing collected packages: tf-estimator-nightly, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.12.0\n",
      "    Uninstalling keras-2.12.0:\n",
      "      Successfully uninstalled keras-2.12.0\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.0\n",
      "    Uninstalling tensorboard-data-server-0.7.0:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 1.0.0\n",
      "    Uninstalling google-auth-oauthlib-1.0.0:\n",
      "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.12.2\n",
      "    Uninstalling tensorboard-2.12.2:\n",
      "      Successfully uninstalled tensorboard-2.12.2\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.12.0\n",
      "    Uninstalling tensorflow-2.12.0:\n",
      "      Successfully uninstalled tensorflow-2.12.0\n",
      "Successfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
     ]
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install tensorflow==2.8\n",
    "else:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install pyglet==1.5.0\n",
    "  %pip install h5py==3.1.0\n",
    "  %pip install Pillow==9.5.0\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install Keras==2.2.4\n",
    "  %pip install tensorflow==2.5.3\n",
    "  %pip install torch==2.0.1\n",
    "  %pip install agents==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hzP_5ZuGb2X"
   },
   "source": [
    "---\n",
    "## **PARTE 2**. Enunciado\n",
    "\n",
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, una solución óptima será alcanzada cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
    "\n",
    "Este proyecto práctico consta de tres partes:\n",
    "\n",
    "1.   Implementar la red neuronal que se usará en la solución\n",
    "2.   Implementar las distintas piezas de la solución DQN\n",
    "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_b3mzw8IzJP"
   },
   "source": [
    "---\n",
    "## **PARTE 3**. Desarrollo y preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duPmUNOVGb2a"
   },
   "source": [
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3eRhgI-Gb2a"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4jgQjzoGb2a"
   },
   "source": [
    "#### Configuración base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwOE6I_KGb2a"
   },
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9jGEZUcpGb2a"
   },
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yitXTADGb2b"
   },
   "source": [
    "1. Implementación de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1687296295682,
     "user": {
      "displayName": "Anton S",
      "userId": "11207087415458905114"
     },
     "user_tz": -120
    },
    "id": "O4GKrfWSGb2b",
    "outputId": "582f1020-fa5e-48bd-84a5-f4fac1b5207c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " permute (Permute)           (None, 84, 84, 4)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 20, 20, 32)        8224      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 9, 9, 64)          32832     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1606144   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,816,998\n",
      "Trainable params: 1,816,998\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE # las 4 imagenes que forman un estado se \"apilan\" en canales\n",
    "model = Sequential()\n",
    "print(K.image_data_format())\n",
    "if K.image_data_format() == 'channels_last':\n",
    "    # (width, height, channels)\n",
    "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "elif K.image_data_format() == 'channels_first':\n",
    "    # (channels, width, height)\n",
    "    model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_dim_ordering.')\n",
    "\n",
    "# IMPLEMENTAR ARQUITECTURA AQUI!!!\n",
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4), activation='relu'))\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(nb_actions, activation='linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB9-_5HPGb2b"
   },
   "source": [
    "2. Implementación de la solución DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foSlxWH1Gb2b"
   },
   "outputs": [],
   "source": [
    "processor = AtariProcessor()\n",
    "\n",
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
    "                                  value_max=1., value_min=.1, value_test=.05,\n",
    "                                  nb_steps=1000000)\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy,\n",
    "                memory=memory, processor=processor,\n",
    "                nb_steps_warmup=50000, gamma=.99,\n",
    "                target_model_update=2000,\n",
    "                train_interval=8)\n",
    "\n",
    "dqn.compile(Adam(learning_rate=1e-4), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f70cb82b",
    "outputId": "8fec2ff8-f306-49bc-8809-9ac05fd4dc39",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2500000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   26/10000 [..............................] - ETA: 41s - reward: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\anaconda3\\envs\\reinforcement_learning\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.0144\n",
      "13 episodes - episode_reward: 11.077 [6.000, 19.000] - ale.lives: 2.009\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.0135\n",
      "16 episodes - episode_reward: 8.438 [4.000, 14.000] - ale.lives: 2.111\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.0111\n",
      "14 episodes - episode_reward: 7.643 [2.000, 16.000] - ale.lives: 2.153\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.0126\n",
      "12 episodes - episode_reward: 10.333 [3.000, 17.000] - ale.lives: 2.035\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.0116\n",
      "14 episodes - episode_reward: 8.571 [2.000, 18.000] - ale.lives: 1.970\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 177s 18ms/step - reward: 0.0135\n",
      "16 episodes - episode_reward: 8.375 [2.000, 14.000] - loss: 0.006 - mae: 0.078 - mean_q: 0.105 - mean_eps: 0.951 - ale.lives: 2.064\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 177s 18ms/step - reward: 0.0128\n",
      "16 episodes - episode_reward: 7.875 [4.000, 13.000] - loss: 0.006 - mae: 0.182 - mean_q: 0.229 - mean_eps: 0.942 - ale.lives: 2.120\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 178s 18ms/step - reward: 0.0142\n",
      "15 episodes - episode_reward: 9.533 [4.000, 15.000] - loss: 0.007 - mae: 0.331 - mean_q: 0.409 - mean_eps: 0.933 - ale.lives: 2.121\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 180s 18ms/step - reward: 0.0134\n",
      "16 episodes - episode_reward: 8.375 [4.000, 15.000] - loss: 0.007 - mae: 0.483 - mean_q: 0.591 - mean_eps: 0.924 - ale.lives: 2.106\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 181s 18ms/step - reward: 0.0142\n",
      "12 episodes - episode_reward: 11.250 [4.000, 20.000] - loss: 0.007 - mae: 0.550 - mean_q: 0.671 - mean_eps: 0.915 - ale.lives: 2.050\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 182s 18ms/step - reward: 0.0116\n",
      "12 episodes - episode_reward: 10.583 [6.000, 17.000] - loss: 0.007 - mae: 0.649 - mean_q: 0.789 - mean_eps: 0.906 - ale.lives: 2.048\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 185s 19ms/step - reward: 0.0135\n",
      "16 episodes - episode_reward: 8.438 [2.000, 23.000] - loss: 0.008 - mae: 0.729 - mean_q: 0.884 - mean_eps: 0.897 - ale.lives: 2.196\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 186s 19ms/step - reward: 0.0133\n",
      "14 episodes - episode_reward: 9.500 [2.000, 16.000] - loss: 0.008 - mae: 0.765 - mean_q: 0.928 - mean_eps: 0.888 - ale.lives: 2.112\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 187s 19ms/step - reward: 0.0149\n",
      "11 episodes - episode_reward: 13.364 [7.000, 24.000] - loss: 0.008 - mae: 0.855 - mean_q: 1.037 - mean_eps: 0.879 - ale.lives: 1.884\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 190s 19ms/step - reward: 0.0154\n",
      "15 episodes - episode_reward: 10.000 [3.000, 25.000] - loss: 0.009 - mae: 0.952 - mean_q: 1.154 - mean_eps: 0.870 - ale.lives: 2.083\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 192s 19ms/step - reward: 0.0126\n",
      "12 episodes - episode_reward: 10.583 [7.000, 19.000] - loss: 0.010 - mae: 1.138 - mean_q: 1.377 - mean_eps: 0.861 - ale.lives: 2.063\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 193s 19ms/step - reward: 0.0142\n",
      "15 episodes - episode_reward: 9.400 [3.000, 25.000] - loss: 0.011 - mae: 1.254 - mean_q: 1.516 - mean_eps: 0.852 - ale.lives: 2.005\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 196s 20ms/step - reward: 0.0144\n",
      "13 episodes - episode_reward: 11.538 [4.000, 26.000] - loss: 0.010 - mae: 1.302 - mean_q: 1.574 - mean_eps: 0.843 - ale.lives: 2.005\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 200s 20ms/step - reward: 0.0127\n",
      "12 episodes - episode_reward: 10.083 [5.000, 23.000] - loss: 0.009 - mae: 1.330 - mean_q: 1.605 - mean_eps: 0.834 - ale.lives: 2.091\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 201s 20ms/step - reward: 0.0144\n",
      "14 episodes - episode_reward: 9.500 [2.000, 20.000] - loss: 0.010 - mae: 1.409 - mean_q: 1.701 - mean_eps: 0.825 - ale.lives: 1.901\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 202s 20ms/step - reward: 0.0159\n",
      "11 episodes - episode_reward: 15.545 [5.000, 27.000] - loss: 0.010 - mae: 1.417 - mean_q: 1.711 - mean_eps: 0.816 - ale.lives: 2.165\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 204s 20ms/step - reward: 0.0173\n",
      "12 episodes - episode_reward: 13.250 [6.000, 21.000] - loss: 0.010 - mae: 1.427 - mean_q: 1.721 - mean_eps: 0.807 - ale.lives: 2.204\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 207s 21ms/step - reward: 0.0139\n",
      "14 episodes - episode_reward: 10.857 [1.000, 33.000] - loss: 0.011 - mae: 1.487 - mean_q: 1.794 - mean_eps: 0.798 - ale.lives: 2.048\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: 0.0146\n",
      "13 episodes - episode_reward: 11.077 [2.000, 20.000] - loss: 0.011 - mae: 1.507 - mean_q: 1.818 - mean_eps: 0.789 - ale.lives: 2.084\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: 0.0156\n",
      "14 episodes - episode_reward: 10.857 [5.000, 20.000] - loss: 0.011 - mae: 1.552 - mean_q: 1.871 - mean_eps: 0.780 - ale.lives: 2.010\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: 0.0156\n",
      "14 episodes - episode_reward: 11.429 [4.000, 26.000] - loss: 0.011 - mae: 1.648 - mean_q: 1.988 - mean_eps: 0.771 - ale.lives: 2.186\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0158\n",
      "13 episodes - episode_reward: 12.308 [7.000, 18.000] - loss: 0.012 - mae: 1.733 - mean_q: 2.091 - mean_eps: 0.762 - ale.lives: 2.057\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0173\n",
      "14 episodes - episode_reward: 12.143 [6.000, 26.000] - loss: 0.011 - mae: 1.774 - mean_q: 2.139 - mean_eps: 0.753 - ale.lives: 2.042\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0152\n",
      "16 episodes - episode_reward: 9.750 [2.000, 18.000] - loss: 0.013 - mae: 1.858 - mean_q: 2.240 - mean_eps: 0.744 - ale.lives: 2.073\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0171\n",
      "14 episodes - episode_reward: 12.143 [4.000, 19.000] - loss: 0.012 - mae: 1.932 - mean_q: 2.329 - mean_eps: 0.735 - ale.lives: 2.035\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0185\n",
      "13 episodes - episode_reward: 14.692 [9.000, 24.000] - loss: 0.012 - mae: 1.945 - mean_q: 2.345 - mean_eps: 0.726 - ale.lives: 2.171\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 229s 23ms/step - reward: 0.0152\n",
      "16 episodes - episode_reward: 9.000 [3.000, 16.000] - loss: 0.012 - mae: 1.968 - mean_q: 2.373 - mean_eps: 0.717 - ale.lives: 2.089\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 235s 23ms/step - reward: 0.0159\n",
      "14 episodes - episode_reward: 11.857 [4.000, 23.000] - loss: 0.013 - mae: 1.974 - mean_q: 2.380 - mean_eps: 0.708 - ale.lives: 2.138\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 236s 24ms/step - reward: 0.0187\n",
      "15 episodes - episode_reward: 12.133 [4.000, 26.000] - loss: 0.013 - mae: 2.082 - mean_q: 2.509 - mean_eps: 0.699 - ale.lives: 2.176\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 235s 24ms/step - reward: 0.0158\n",
      "16 episodes - episode_reward: 10.250 [5.000, 23.000] - loss: 0.012 - mae: 2.143 - mean_q: 2.584 - mean_eps: 0.690 - ale.lives: 1.998\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 240s 24ms/step - reward: 0.0178\n",
      "15 episodes - episode_reward: 11.867 [5.000, 26.000] - loss: 0.012 - mae: 2.137 - mean_q: 2.578 - mean_eps: 0.681 - ale.lives: 1.968\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 245s 24ms/step - reward: 0.0165\n",
      "13 episodes - episode_reward: 12.231 [5.000, 25.000] - loss: 0.013 - mae: 2.218 - mean_q: 2.675 - mean_eps: 0.672 - ale.lives: 1.991\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 246s 25ms/step - reward: 0.0172\n",
      "16 episodes - episode_reward: 10.625 [3.000, 21.000] - loss: 0.014 - mae: 2.316 - mean_q: 2.791 - mean_eps: 0.663 - ale.lives: 2.042\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 250s 25ms/step - reward: 0.0163\n",
      "16 episodes - episode_reward: 10.500 [5.000, 19.000] - loss: 0.013 - mae: 2.284 - mean_q: 2.755 - mean_eps: 0.654 - ale.lives: 2.079\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 253s 25ms/step - reward: 0.0172\n",
      "14 episodes - episode_reward: 11.786 [2.000, 21.000] - loss: 0.014 - mae: 2.283 - mean_q: 2.752 - mean_eps: 0.645 - ale.lives: 2.133\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 253s 25ms/step - reward: 0.0160\n",
      "14 episodes - episode_reward: 12.071 [5.000, 25.000] - loss: 0.013 - mae: 2.304 - mean_q: 2.778 - mean_eps: 0.636 - ale.lives: 2.066\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 253s 25ms/step - reward: 0.0186\n",
      "11 episodes - episode_reward: 16.000 [5.000, 30.000] - loss: 0.013 - mae: 2.376 - mean_q: 2.862 - mean_eps: 0.627 - ale.lives: 2.213\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 256s 26ms/step - reward: 0.0156\n",
      "14 episodes - episode_reward: 11.857 [6.000, 22.000] - loss: 0.014 - mae: 2.470 - mean_q: 2.976 - mean_eps: 0.618 - ale.lives: 2.079\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 264s 26ms/step - reward: 0.0164\n",
      "14 episodes - episode_reward: 11.357 [3.000, 19.000] - loss: 0.014 - mae: 2.493 - mean_q: 3.003 - mean_eps: 0.609 - ale.lives: 2.202\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 269s 27ms/step - reward: 0.0149\n",
      "11 episodes - episode_reward: 13.545 [5.000, 26.000] - loss: 0.014 - mae: 2.567 - mean_q: 3.093 - mean_eps: 0.600 - ale.lives: 2.198\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 265s 26ms/step - reward: 0.0184\n",
      "12 episodes - episode_reward: 14.500 [4.000, 30.000] - loss: 0.014 - mae: 2.606 - mean_q: 3.138 - mean_eps: 0.591 - ale.lives: 2.038\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 264s 26ms/step - reward: 0.0174\n",
      "14 episodes - episode_reward: 13.571 [5.000, 26.000] - loss: 0.014 - mae: 2.658 - mean_q: 3.203 - mean_eps: 0.582 - ale.lives: 2.087\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 263s 26ms/step - reward: 0.0155\n",
      "15 episodes - episode_reward: 9.600 [3.000, 19.000] - loss: 0.013 - mae: 2.629 - mean_q: 3.167 - mean_eps: 0.573 - ale.lives: 2.119\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 264s 26ms/step - reward: 0.0155\n",
      "15 episodes - episode_reward: 11.067 [7.000, 16.000] - loss: 0.012 - mae: 2.581 - mean_q: 3.110 - mean_eps: 0.564 - ale.lives: 2.120\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0172\n",
      "12 episodes - episode_reward: 13.417 [4.000, 21.000] - loss: 0.013 - mae: 2.581 - mean_q: 3.111 - mean_eps: 0.555 - ale.lives: 2.141\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 269s 27ms/step - reward: 0.0175\n",
      "13 episodes - episode_reward: 13.308 [8.000, 26.000] - loss: 0.012 - mae: 2.567 - mean_q: 3.093 - mean_eps: 0.546 - ale.lives: 2.207\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 272s 27ms/step - reward: 0.0182\n",
      "13 episodes - episode_reward: 13.538 [6.000, 29.000] - loss: 0.013 - mae: 2.568 - mean_q: 3.092 - mean_eps: 0.537 - ale.lives: 2.124\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 271s 27ms/step - reward: 0.0173\n",
      "13 episodes - episode_reward: 14.769 [6.000, 30.000] - loss: 0.013 - mae: 2.583 - mean_q: 3.113 - mean_eps: 0.528 - ale.lives: 2.114\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: 0.0160\n",
      "13 episodes - episode_reward: 11.615 [4.000, 19.000] - loss: 0.013 - mae: 2.622 - mean_q: 3.159 - mean_eps: 0.519 - ale.lives: 2.054\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0146\n",
      "13 episodes - episode_reward: 10.769 [2.000, 21.000] - loss: 0.013 - mae: 2.589 - mean_q: 3.118 - mean_eps: 0.510 - ale.lives: 2.110\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 280s 28ms/step - reward: 0.0159\n",
      "14 episodes - episode_reward: 12.429 [4.000, 25.000] - loss: 0.012 - mae: 2.626 - mean_q: 3.163 - mean_eps: 0.501 - ale.lives: 2.129\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 282s 28ms/step - reward: 0.0146\n",
      "13 episodes - episode_reward: 10.462 [5.000, 19.000] - loss: 0.013 - mae: 2.705 - mean_q: 3.258 - mean_eps: 0.492 - ale.lives: 2.139\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 287s 29ms/step - reward: 0.0163\n",
      "11 episodes - episode_reward: 14.364 [5.000, 22.000] - loss: 0.014 - mae: 2.824 - mean_q: 3.401 - mean_eps: 0.483 - ale.lives: 2.050\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 289s 29ms/step - reward: 0.0164\n",
      "14 episodes - episode_reward: 12.714 [4.000, 24.000] - loss: 0.014 - mae: 2.856 - mean_q: 3.439 - mean_eps: 0.474 - ale.lives: 2.048\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0169\n",
      "14 episodes - episode_reward: 11.714 [5.000, 31.000] - loss: 0.014 - mae: 2.826 - mean_q: 3.403 - mean_eps: 0.465 - ale.lives: 1.988\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 314s 31ms/step - reward: 0.0174\n",
      "14 episodes - episode_reward: 12.500 [3.000, 25.000] - loss: 0.015 - mae: 2.877 - mean_q: 3.465 - mean_eps: 0.456 - ale.lives: 2.174\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 317s 32ms/step - reward: 0.0160\n",
      "13 episodes - episode_reward: 11.923 [6.000, 20.000] - loss: 0.013 - mae: 2.912 - mean_q: 3.505 - mean_eps: 0.447 - ale.lives: 2.067\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 307s 31ms/step - reward: 0.0148\n",
      "14 episodes - episode_reward: 11.214 [5.000, 20.000] - loss: 0.014 - mae: 2.951 - mean_q: 3.552 - mean_eps: 0.438 - ale.lives: 2.173\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 304s 30ms/step - reward: 0.0166\n",
      "14 episodes - episode_reward: 11.786 [4.000, 20.000] - loss: 0.014 - mae: 2.996 - mean_q: 3.606 - mean_eps: 0.429 - ale.lives: 2.151\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 306s 31ms/step - reward: 0.0196\n",
      "14 episodes - episode_reward: 14.000 [6.000, 23.000] - loss: 0.015 - mae: 3.062 - mean_q: 3.688 - mean_eps: 0.420 - ale.lives: 2.153\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0178\n",
      "14 episodes - episode_reward: 12.571 [7.000, 23.000] - loss: 0.016 - mae: 3.175 - mean_q: 3.825 - mean_eps: 0.411 - ale.lives: 2.009\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0183\n",
      "13 episodes - episode_reward: 13.077 [2.000, 19.000] - loss: 0.016 - mae: 3.229 - mean_q: 3.889 - mean_eps: 0.402 - ale.lives: 1.911\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0178\n",
      "13 episodes - episode_reward: 14.462 [7.000, 24.000] - loss: 0.016 - mae: 3.271 - mean_q: 3.937 - mean_eps: 0.393 - ale.lives: 2.072\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 316s 32ms/step - reward: 0.0181\n",
      "14 episodes - episode_reward: 13.429 [6.000, 27.000] - loss: 0.016 - mae: 3.234 - mean_q: 3.895 - mean_eps: 0.384 - ale.lives: 1.989\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 316s 32ms/step - reward: 0.0183\n",
      "12 episodes - episode_reward: 15.000 [4.000, 27.000] - loss: 0.016 - mae: 3.184 - mean_q: 3.833 - mean_eps: 0.375 - ale.lives: 2.088\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0174\n",
      "13 episodes - episode_reward: 13.000 [7.000, 22.000] - loss: 0.016 - mae: 3.113 - mean_q: 3.749 - mean_eps: 0.366 - ale.lives: 2.189\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0167\n",
      "12 episodes - episode_reward: 13.750 [5.000, 22.000] - loss: 0.016 - mae: 3.218 - mean_q: 3.876 - mean_eps: 0.357 - ale.lives: 2.379\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0191\n",
      "13 episodes - episode_reward: 13.846 [5.000, 26.000] - loss: 0.017 - mae: 3.282 - mean_q: 3.954 - mean_eps: 0.348 - ale.lives: 2.030\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 326s 33ms/step - reward: 0.0178\n",
      "14 episodes - episode_reward: 14.214 [4.000, 30.000] - loss: 0.019 - mae: 3.381 - mean_q: 4.075 - mean_eps: 0.339 - ale.lives: 2.105\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "10000/10000 [==============================] - 330s 33ms/step - reward: 0.0185\n",
      "11 episodes - episode_reward: 14.818 [5.000, 19.000] - loss: 0.018 - mae: 3.391 - mean_q: 4.085 - mean_eps: 0.330 - ale.lives: 2.034\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 334s 33ms/step - reward: 0.0172\n",
      "14 episodes - episode_reward: 13.286 [4.000, 23.000] - loss: 0.019 - mae: 3.361 - mean_q: 4.049 - mean_eps: 0.321 - ale.lives: 2.128\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      " 8881/10000 [=========================>....] - ETA: 36s - reward: 0.0185"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 411s 41ms/step - reward: 0.0186\n",
      "12 episodes - episode_reward: 16.417 [7.000, 24.000] - loss: 0.017 - mae: 3.360 - mean_q: 4.043 - mean_eps: 0.100 - ale.lives: 2.100\n",
      "\n",
      "Interval 134 (1330000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0202\n",
      "10 episodes - episode_reward: 19.400 [11.000, 29.000] - loss: 0.017 - mae: 3.305 - mean_q: 3.979 - mean_eps: 0.100 - ale.lives: 2.255\n",
      "\n",
      "Interval 135 (1340000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0195\n",
      "12 episodes - episode_reward: 16.667 [9.000, 27.000] - loss: 0.017 - mae: 3.305 - mean_q: 3.979 - mean_eps: 0.100 - ale.lives: 2.165\n",
      "\n",
      "Interval 136 (1350000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0196\n",
      "12 episodes - episode_reward: 17.167 [10.000, 24.000] - loss: 0.016 - mae: 3.305 - mean_q: 3.977 - mean_eps: 0.100 - ale.lives: 2.138\n",
      "\n",
      "Interval 137 (1360000 steps performed)\n",
      "10000/10000 [==============================] - 409s 41ms/step - reward: 0.0206\n",
      "12 episodes - episode_reward: 17.250 [8.000, 22.000] - loss: 0.016 - mae: 3.176 - mean_q: 3.821 - mean_eps: 0.100 - ale.lives: 1.915\n",
      "\n",
      "Interval 138 (1370000 steps performed)\n",
      "10000/10000 [==============================] - 416s 42ms/step - reward: 0.0219\n",
      "11 episodes - episode_reward: 19.000 [8.000, 31.000] - loss: 0.015 - mae: 3.207 - mean_q: 3.859 - mean_eps: 0.100 - ale.lives: 2.313\n",
      "\n",
      "Interval 139 (1380000 steps performed)\n",
      "10000/10000 [==============================] - 416s 42ms/step - reward: 0.0197\n",
      "14 episodes - episode_reward: 13.857 [5.000, 26.000] - loss: 0.016 - mae: 3.204 - mean_q: 3.856 - mean_eps: 0.100 - ale.lives: 2.088\n",
      "\n",
      "Interval 140 (1390000 steps performed)\n",
      "10000/10000 [==============================] - 415s 41ms/step - reward: 0.0185\n",
      "14 episodes - episode_reward: 13.500 [7.000, 19.000] - loss: 0.015 - mae: 3.224 - mean_q: 3.880 - mean_eps: 0.100 - ale.lives: 2.188\n",
      "\n",
      "Interval 141 (1400000 steps performed)\n",
      "10000/10000 [==============================] - 414s 41ms/step - reward: 0.0201\n",
      "11 episodes - episode_reward: 18.636 [14.000, 28.000] - loss: 0.016 - mae: 3.261 - mean_q: 3.925 - mean_eps: 0.100 - ale.lives: 2.073\n",
      "\n",
      "Interval 142 (1410000 steps performed)\n",
      "10000/10000 [==============================] - 415s 41ms/step - reward: 0.0194\n",
      "13 episodes - episode_reward: 15.077 [3.000, 24.000] - loss: 0.016 - mae: 3.300 - mean_q: 3.974 - mean_eps: 0.100 - ale.lives: 2.130\n",
      "\n",
      "Interval 143 (1420000 steps performed)\n",
      "10000/10000 [==============================] - 413s 41ms/step - reward: 0.0204\n",
      "12 episodes - episode_reward: 16.583 [8.000, 23.000] - loss: 0.015 - mae: 3.303 - mean_q: 3.977 - mean_eps: 0.100 - ale.lives: 2.110\n",
      "\n",
      "Interval 144 (1430000 steps performed)\n",
      "10000/10000 [==============================] - 414s 41ms/step - reward: 0.0196\n",
      "12 episodes - episode_reward: 16.583 [11.000, 26.000] - loss: 0.016 - mae: 3.301 - mean_q: 3.972 - mean_eps: 0.100 - ale.lives: 2.150\n",
      "\n",
      "Interval 145 (1440000 steps performed)\n",
      "10000/10000 [==============================] - 413s 41ms/step - reward: 0.0198\n",
      "10 episodes - episode_reward: 18.600 [9.000, 27.000] - loss: 0.015 - mae: 3.318 - mean_q: 3.991 - mean_eps: 0.100 - ale.lives: 2.209\n",
      "\n",
      "Interval 146 (1450000 steps performed)\n",
      " 9489/10000 [===========================>..] - ETA: 20s - reward: 0.0189"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 419s 42ms/step - reward: 0.0225\n",
      "12 episodes - episode_reward: 19.333 [9.000, 35.000] - loss: 0.018 - mae: 3.377 - mean_q: 4.066 - mean_eps: 0.100 - ale.lives: 2.081\n",
      "\n",
      "Interval 178 (1770000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0216\n",
      "14 episodes - episode_reward: 15.500 [7.000, 23.000] - loss: 0.017 - mae: 3.294 - mean_q: 3.966 - mean_eps: 0.100 - ale.lives: 2.054\n",
      "\n",
      "Interval 179 (1780000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0214\n",
      "12 episodes - episode_reward: 17.250 [9.000, 30.000] - loss: 0.018 - mae: 3.270 - mean_q: 3.939 - mean_eps: 0.100 - ale.lives: 2.219\n",
      "\n",
      "Interval 180 (1790000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0204\n",
      "14 episodes - episode_reward: 15.071 [7.000, 21.000] - loss: 0.018 - mae: 3.276 - mean_q: 3.947 - mean_eps: 0.100 - ale.lives: 2.271\n",
      "\n",
      "Interval 181 (1800000 steps performed)\n",
      "10000/10000 [==============================] - 417s 42ms/step - reward: 0.0211\n",
      "14 episodes - episode_reward: 14.929 [5.000, 23.000] - loss: 0.020 - mae: 3.276 - mean_q: 3.945 - mean_eps: 0.100 - ale.lives: 2.160\n",
      "\n",
      "Interval 182 (1810000 steps performed)\n",
      "10000/10000 [==============================] - 416s 42ms/step - reward: 0.0231\n",
      "13 episodes - episode_reward: 17.462 [6.000, 30.000] - loss: 0.020 - mae: 3.307 - mean_q: 3.982 - mean_eps: 0.100 - ale.lives: 2.161\n",
      "\n",
      "Interval 183 (1820000 steps performed)\n",
      "10000/10000 [==============================] - 432s 43ms/step - reward: 0.0240\n",
      "13 episodes - episode_reward: 19.308 [10.000, 28.000] - loss: 0.018 - mae: 3.377 - mean_q: 4.067 - mean_eps: 0.100 - ale.lives: 2.101\n",
      "\n",
      "Interval 184 (1830000 steps performed)\n",
      "10000/10000 [==============================] - 484s 48ms/step - reward: 0.0240\n",
      "12 episodes - episode_reward: 19.250 [12.000, 30.000] - loss: 0.020 - mae: 3.442 - mean_q: 4.146 - mean_eps: 0.100 - ale.lives: 2.238\n",
      "\n",
      "Interval 185 (1840000 steps performed)\n",
      "10000/10000 [==============================] - 490s 49ms/step - reward: 0.0202\n",
      "13 episodes - episode_reward: 15.846 [4.000, 27.000] - loss: 0.019 - mae: 3.445 - mean_q: 4.148 - mean_eps: 0.100 - ale.lives: 1.956\n",
      "\n",
      "Interval 186 (1850000 steps performed)\n",
      "10000/10000 [==============================] - 482s 48ms/step - reward: 0.0224\n",
      "13 episodes - episode_reward: 17.154 [8.000, 31.000] - loss: 0.019 - mae: 3.433 - mean_q: 4.131 - mean_eps: 0.100 - ale.lives: 2.181\n",
      "\n",
      "Interval 187 (1860000 steps performed)\n",
      "10000/10000 [==============================] - 486s 49ms/step - reward: 0.0232\n",
      "13 episodes - episode_reward: 17.231 [10.000, 26.000] - loss: 0.019 - mae: 3.382 - mean_q: 4.072 - mean_eps: 0.100 - ale.lives: 2.001\n",
      "\n",
      "Interval 188 (1870000 steps performed)\n",
      "10000/10000 [==============================] - 484s 48ms/step - reward: 0.0234\n",
      "13 episodes - episode_reward: 18.692 [8.000, 31.000] - loss: 0.019 - mae: 3.400 - mean_q: 4.092 - mean_eps: 0.100 - ale.lives: 1.904\n",
      "\n",
      "Interval 189 (1880000 steps performed)\n",
      "10000/10000 [==============================] - 485s 49ms/step - reward: 0.0228\n",
      "13 episodes - episode_reward: 17.077 [6.000, 29.000] - loss: 0.018 - mae: 3.357 - mean_q: 4.041 - mean_eps: 0.100 - ale.lives: 2.029\n",
      "\n",
      "Interval 190 (1890000 steps performed)\n",
      "10000/10000 [==============================] - 490s 49ms/step - reward: 0.0220\n",
      "10 episodes - episode_reward: 22.300 [8.000, 35.000] - loss: 0.018 - mae: 3.348 - mean_q: 4.030 - mean_eps: 0.100 - ale.lives: 1.994\n",
      "\n",
      "Interval 191 (1900000 steps performed)\n",
      "10000/10000 [==============================] - 485s 48ms/step - reward: 0.0234\n",
      "12 episodes - episode_reward: 19.250 [8.000, 35.000] - loss: 0.016 - mae: 3.352 - mean_q: 4.035 - mean_eps: 0.100 - ale.lives: 2.199\n",
      "\n",
      "Interval 192 (1910000 steps performed)\n",
      "10000/10000 [==============================] - 478s 48ms/step - reward: 0.0229\n",
      "14 episodes - episode_reward: 16.571 [10.000, 32.000] - loss: 0.019 - mae: 3.431 - mean_q: 4.132 - mean_eps: 0.100 - ale.lives: 1.942\n",
      "\n",
      "Interval 193 (1920000 steps performed)\n",
      "10000/10000 [==============================] - 418s 42ms/step - reward: 0.0197\n",
      "12 episodes - episode_reward: 16.000 [5.000, 27.000] - loss: 0.018 - mae: 3.468 - mean_q: 4.176 - mean_eps: 0.100 - ale.lives: 2.088\n",
      "\n",
      "Interval 194 (1930000 steps performed)\n",
      "10000/10000 [==============================] - 456s 46ms/step - reward: 0.0214\n",
      "12 episodes - episode_reward: 18.250 [10.000, 28.000] - loss: 0.018 - mae: 3.499 - mean_q: 4.212 - mean_eps: 0.100 - ale.lives: 2.059\n",
      "\n",
      "Interval 195 (1940000 steps performed)\n",
      "10000/10000 [==============================] - 476s 48ms/step - reward: 0.0226\n",
      "12 episodes - episode_reward: 18.500 [8.000, 27.000] - loss: 0.019 - mae: 3.518 - mean_q: 4.234 - mean_eps: 0.100 - ale.lives: 2.113\n",
      "\n",
      "Interval 196 (1950000 steps performed)\n",
      "10000/10000 [==============================] - 452s 45ms/step - reward: 0.0211\n",
      "14 episodes - episode_reward: 15.857 [6.000, 31.000] - loss: 0.018 - mae: 3.460 - mean_q: 4.164 - mean_eps: 0.100 - ale.lives: 2.174\n",
      "\n",
      "Interval 197 (1960000 steps performed)\n",
      "10000/10000 [==============================] - 451s 45ms/step - reward: 0.0248\n",
      "13 episodes - episode_reward: 18.308 [11.000, 25.000] - loss: 0.018 - mae: 3.436 - mean_q: 4.135 - mean_eps: 0.100 - ale.lives: 2.048\n",
      "\n",
      "Interval 198 (1970000 steps performed)\n",
      "10000/10000 [==============================] - 453s 45ms/step - reward: 0.0229\n",
      "14 episodes - episode_reward: 16.214 [7.000, 25.000] - loss: 0.018 - mae: 3.460 - mean_q: 4.164 - mean_eps: 0.100 - ale.lives: 2.166\n",
      "\n",
      "Interval 199 (1980000 steps performed)\n",
      "10000/10000 [==============================] - 449s 45ms/step - reward: 0.0217\n",
      "11 episodes - episode_reward: 20.000 [6.000, 32.000] - loss: 0.017 - mae: 3.424 - mean_q: 4.122 - mean_eps: 0.100 - ale.lives: 1.906\n",
      "\n",
      "Interval 200 (1990000 steps performed)\n",
      "10000/10000 [==============================] - 422s 42ms/step - reward: 0.0221\n",
      "12 episodes - episode_reward: 19.250 [12.000, 28.000] - loss: 0.017 - mae: 3.377 - mean_q: 4.063 - mean_eps: 0.100 - ale.lives: 2.107\n",
      "\n",
      "Interval 201 (2000000 steps performed)\n",
      "10000/10000 [==============================] - 452s 45ms/step - reward: 0.0229\n",
      "13 episodes - episode_reward: 15.692 [6.000, 34.000] - loss: 0.017 - mae: 3.412 - mean_q: 4.106 - mean_eps: 0.100 - ale.lives: 1.971\n",
      "\n",
      "Interval 202 (2010000 steps performed)\n",
      "10000/10000 [==============================] - 461s 46ms/step - reward: 0.0228\n",
      "14 episodes - episode_reward: 18.071 [7.000, 30.000] - loss: 0.018 - mae: 3.447 - mean_q: 4.149 - mean_eps: 0.100 - ale.lives: 1.997\n",
      "\n",
      "Interval 203 (2020000 steps performed)\n",
      "10000/10000 [==============================] - 442s 44ms/step - reward: 0.0236\n",
      "14 episodes - episode_reward: 16.857 [9.000, 27.000] - loss: 0.018 - mae: 3.471 - mean_q: 4.180 - mean_eps: 0.100 - ale.lives: 2.206\n",
      "\n",
      "Interval 204 (2030000 steps performed)\n",
      "10000/10000 [==============================] - 437s 44ms/step - reward: 0.0246\n",
      "12 episodes - episode_reward: 19.500 [11.000, 32.000] - loss: 0.018 - mae: 3.467 - mean_q: 4.176 - mean_eps: 0.100 - ale.lives: 2.065\n",
      "\n",
      "Interval 205 (2040000 steps performed)\n",
      "10000/10000 [==============================] - 423s 42ms/step - reward: 0.0220\n",
      "13 episodes - episode_reward: 16.692 [2.000, 24.000] - loss: 0.020 - mae: 3.440 - mean_q: 4.143 - mean_eps: 0.100 - ale.lives: 2.071\n",
      "\n",
      "Interval 206 (2050000 steps performed)\n",
      "10000/10000 [==============================] - 414s 41ms/step - reward: 0.0242\n",
      "12 episodes - episode_reward: 20.500 [13.000, 29.000] - loss: 0.020 - mae: 3.448 - mean_q: 4.153 - mean_eps: 0.100 - ale.lives: 2.059\n",
      "\n",
      "Interval 207 (2060000 steps performed)\n",
      "10000/10000 [==============================] - 414s 41ms/step - reward: 0.0235\n",
      "13 episodes - episode_reward: 18.615 [12.000, 25.000] - loss: 0.020 - mae: 3.432 - mean_q: 4.131 - mean_eps: 0.100 - ale.lives: 2.132\n",
      "\n",
      "Interval 208 (2070000 steps performed)\n",
      "10000/10000 [==============================] - 411s 41ms/step - reward: 0.0247\n",
      "14 episodes - episode_reward: 17.929 [5.000, 27.000] - loss: 0.019 - mae: 3.401 - mean_q: 4.094 - mean_eps: 0.100 - ale.lives: 2.017\n",
      "\n",
      "Interval 209 (2080000 steps performed)\n",
      "10000/10000 [==============================] - 411s 41ms/step - reward: 0.0230\n",
      "12 episodes - episode_reward: 17.667 [8.000, 33.000] - loss: 0.019 - mae: 3.373 - mean_q: 4.061 - mean_eps: 0.100 - ale.lives: 2.125\n",
      "\n",
      "Interval 210 (2090000 steps performed)\n",
      "10000/10000 [==============================] - 412s 41ms/step - reward: 0.0238\n",
      "12 episodes - episode_reward: 20.250 [10.000, 34.000] - loss: 0.020 - mae: 3.380 - mean_q: 4.068 - mean_eps: 0.100 - ale.lives: 2.159\n",
      "\n",
      "Interval 211 (2100000 steps performed)\n",
      "10000/10000 [==============================] - 413s 41ms/step - reward: 0.0201\n",
      "11 episodes - episode_reward: 17.545 [6.000, 23.000] - loss: 0.020 - mae: 3.323 - mean_q: 3.998 - mean_eps: 0.100 - ale.lives: 2.174\n",
      "\n",
      "Interval 212 (2110000 steps performed)\n",
      "10000/10000 [==============================] - 412s 41ms/step - reward: 0.0203\n",
      "13 episodes - episode_reward: 16.231 [6.000, 30.000] - loss: 0.018 - mae: 3.316 - mean_q: 3.990 - mean_eps: 0.100 - ale.lives: 1.977\n",
      "\n",
      "Interval 213 (2120000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0236\n",
      "12 episodes - episode_reward: 19.583 [6.000, 32.000] - loss: 0.017 - mae: 3.318 - mean_q: 3.994 - mean_eps: 0.100 - ale.lives: 2.165\n",
      "\n",
      "Interval 214 (2130000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0228\n",
      "12 episodes - episode_reward: 18.833 [4.000, 28.000] - loss: 0.018 - mae: 3.262 - mean_q: 3.925 - mean_eps: 0.100 - ale.lives: 2.056\n",
      "\n",
      "Interval 215 (2140000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0246\n",
      "13 episodes - episode_reward: 19.308 [4.000, 34.000] - loss: 0.017 - mae: 3.216 - mean_q: 3.871 - mean_eps: 0.100 - ale.lives: 2.110\n",
      "\n",
      "Interval 216 (2150000 steps performed)\n",
      "10000/10000 [==============================] - 409s 41ms/step - reward: 0.0235\n",
      "13 episodes - episode_reward: 18.846 [8.000, 32.000] - loss: 0.017 - mae: 3.197 - mean_q: 3.849 - mean_eps: 0.100 - ale.lives: 1.968\n",
      "\n",
      "Interval 217 (2160000 steps performed)\n",
      "10000/10000 [==============================] - 412s 41ms/step - reward: 0.0257\n",
      "11 episodes - episode_reward: 20.909 [12.000, 29.000] - loss: 0.018 - mae: 3.155 - mean_q: 3.798 - mean_eps: 0.100 - ale.lives: 1.948\n",
      "\n",
      "Interval 218 (2170000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0243\n",
      "13 episodes - episode_reward: 20.308 [11.000, 28.000] - loss: 0.018 - mae: 3.173 - mean_q: 3.820 - mean_eps: 0.100 - ale.lives: 2.028\n",
      "\n",
      "Interval 219 (2180000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0250\n",
      "13 episodes - episode_reward: 19.769 [6.000, 28.000] - loss: 0.017 - mae: 3.197 - mean_q: 3.848 - mean_eps: 0.100 - ale.lives: 1.935\n",
      "\n",
      "Interval 220 (2190000 steps performed)\n",
      "10000/10000 [==============================] - 409s 41ms/step - reward: 0.0219\n",
      "12 episodes - episode_reward: 17.250 [5.000, 24.000] - loss: 0.017 - mae: 3.255 - mean_q: 3.918 - mean_eps: 0.100 - ale.lives: 2.165\n",
      "\n",
      "Interval 221 (2200000 steps performed)\n",
      "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0231\n",
      "12 episodes - episode_reward: 19.333 [5.000, 33.000] - loss: 0.018 - mae: 3.292 - mean_q: 3.963 - mean_eps: 0.100 - ale.lives: 2.039\n",
      "\n",
      "Interval 222 (2210000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0244\n",
      "12 episodes - episode_reward: 19.667 [2.000, 29.000] - loss: 0.019 - mae: 3.295 - mean_q: 3.968 - mean_eps: 0.100 - ale.lives: 1.978\n",
      "\n",
      "Interval 223 (2220000 steps performed)\n",
      "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0227\n",
      "13 episodes - episode_reward: 17.231 [7.000, 32.000] - loss: 0.019 - mae: 3.380 - mean_q: 4.071 - mean_eps: 0.100 - ale.lives: 2.192\n",
      "\n",
      "Interval 224 (2230000 steps performed)\n",
      "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0236\n",
      "13 episodes - episode_reward: 18.923 [12.000, 32.000] - loss: 0.018 - mae: 3.443 - mean_q: 4.148 - mean_eps: 0.100 - ale.lives: 1.979\n",
      "\n",
      "Interval 225 (2240000 steps performed)\n",
      "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0228\n",
      "14 episodes - episode_reward: 16.571 [2.000, 25.000] - loss: 0.017 - mae: 3.405 - mean_q: 4.102 - mean_eps: 0.100 - ale.lives: 2.060\n",
      "\n",
      "Interval 226 (2250000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0242\n",
      "13 episodes - episode_reward: 17.769 [5.000, 31.000] - loss: 0.019 - mae: 3.439 - mean_q: 4.140 - mean_eps: 0.100 - ale.lives: 2.003\n",
      "\n",
      "Interval 227 (2260000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0241\n",
      "13 episodes - episode_reward: 19.308 [10.000, 32.000] - loss: 0.019 - mae: 3.475 - mean_q: 4.182 - mean_eps: 0.100 - ale.lives: 2.041\n",
      "\n",
      "Interval 228 (2270000 steps performed)\n",
      "10000/10000 [==============================] - 409s 41ms/step - reward: 0.0238\n",
      "11 episodes - episode_reward: 20.545 [8.000, 26.000] - loss: 0.020 - mae: 3.507 - mean_q: 4.217 - mean_eps: 0.100 - ale.lives: 2.010\n",
      "\n",
      "Interval 229 (2280000 steps performed)\n",
      "10000/10000 [==============================] - 411s 41ms/step - reward: 0.0241\n",
      "12 episodes - episode_reward: 20.333 [11.000, 35.000] - loss: 0.019 - mae: 3.517 - mean_q: 4.231 - mean_eps: 0.100 - ale.lives: 2.136\n",
      "\n",
      "Interval 230 (2290000 steps performed)\n",
      "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0266\n",
      "11 episodes - episode_reward: 24.364 [16.000, 38.000] - loss: 0.019 - mae: 3.526 - mean_q: 4.243 - mean_eps: 0.100 - ale.lives: 2.163\n",
      "\n",
      "Interval 231 (2300000 steps performed)\n",
      "10000/10000 [==============================] - 412s 41ms/step - reward: 0.0241\n",
      "11 episodes - episode_reward: 23.091 [11.000, 34.000] - loss: 0.020 - mae: 3.449 - mean_q: 4.149 - mean_eps: 0.100 - ale.lives: 2.191\n",
      "\n",
      "Interval 232 (2310000 steps performed)\n",
      "10000/10000 [==============================] - 411s 41ms/step - reward: 0.0244\n",
      "11 episodes - episode_reward: 20.909 [11.000, 32.000] - loss: 0.019 - mae: 3.373 - mean_q: 4.058 - mean_eps: 0.100 - ale.lives: 1.959\n",
      "\n",
      "Interval 233 (2320000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0245\n",
      "13 episodes - episode_reward: 19.077 [8.000, 30.000] - loss: 0.018 - mae: 3.373 - mean_q: 4.059 - mean_eps: 0.100 - ale.lives: 2.006\n",
      "\n",
      "Interval 234 (2330000 steps performed)\n",
      "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0254\n",
      "12 episodes - episode_reward: 21.083 [11.000, 32.000] - loss: 0.019 - mae: 3.374 - mean_q: 4.059 - mean_eps: 0.100 - ale.lives: 2.102\n",
      "\n",
      "Interval 235 (2340000 steps performed)\n",
      "10000/10000 [==============================] - 412s 41ms/step - reward: 0.0243\n",
      "11 episodes - episode_reward: 22.455 [14.000, 35.000] - loss: 0.019 - mae: 3.381 - mean_q: 4.066 - mean_eps: 0.100 - ale.lives: 2.007\n",
      "\n",
      "Interval 236 (2350000 steps performed)\n",
      "10000/10000 [==============================] - 413s 41ms/step - reward: 0.0224\n",
      "10 episodes - episode_reward: 21.500 [13.000, 31.000] - loss: 0.017 - mae: 3.340 - mean_q: 4.019 - mean_eps: 0.100 - ale.lives: 2.081\n",
      "\n",
      "Interval 237 (2360000 steps performed)\n",
      "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0259\n",
      "10 episodes - episode_reward: 27.700 [17.000, 35.000] - loss: 0.018 - mae: 3.267 - mean_q: 3.930 - mean_eps: 0.100 - ale.lives: 2.084\n",
      "\n",
      "Interval 238 (2370000 steps performed)\n",
      "10000/10000 [==============================] - 409s 41ms/step - reward: 0.0212\n",
      "13 episodes - episode_reward: 16.154 [8.000, 30.000] - loss: 0.018 - mae: 3.307 - mean_q: 3.978 - mean_eps: 0.100 - ale.lives: 2.097\n",
      "\n",
      "Interval 239 (2380000 steps performed)\n",
      "10000/10000 [==============================] - 409s 41ms/step - reward: 0.0229\n",
      "12 episodes - episode_reward: 18.833 [5.000, 36.000] - loss: 0.018 - mae: 3.338 - mean_q: 4.017 - mean_eps: 0.100 - ale.lives: 2.004\n",
      "\n",
      "Interval 240 (2390000 steps performed)\n",
      "10000/10000 [==============================] - 409s 41ms/step - reward: 0.0239\n",
      "11 episodes - episode_reward: 20.636 [13.000, 31.000] - loss: 0.019 - mae: 3.288 - mean_q: 3.954 - mean_eps: 0.100 - ale.lives: 2.207\n",
      "\n",
      "Interval 241 (2400000 steps performed)\n",
      "10000/10000 [==============================] - 409s 41ms/step - reward: 0.0241\n",
      "12 episodes - episode_reward: 20.917 [7.000, 33.000] - loss: 0.018 - mae: 3.280 - mean_q: 3.946 - mean_eps: 0.100 - ale.lives: 2.145\n",
      "\n",
      "Interval 242 (2410000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0246\n",
      "13 episodes - episode_reward: 19.077 [6.000, 33.000] - loss: 0.018 - mae: 3.289 - mean_q: 3.956 - mean_eps: 0.100 - ale.lives: 2.168\n",
      "\n",
      "Interval 243 (2420000 steps performed)\n",
      "10000/10000 [==============================] - 411s 41ms/step - reward: 0.0218\n",
      "13 episodes - episode_reward: 16.385 [8.000, 24.000] - loss: 0.017 - mae: 3.294 - mean_q: 3.962 - mean_eps: 0.100 - ale.lives: 2.086\n",
      "\n",
      "Interval 244 (2430000 steps performed)\n",
      "10000/10000 [==============================] - 411s 41ms/step - reward: 0.0253\n",
      "13 episodes - episode_reward: 18.923 [9.000, 24.000] - loss: 0.018 - mae: 3.275 - mean_q: 3.941 - mean_eps: 0.100 - ale.lives: 2.062\n",
      "\n",
      "Interval 245 (2440000 steps performed)\n",
      "10000/10000 [==============================] - 409s 41ms/step - reward: 0.0241\n",
      "12 episodes - episode_reward: 20.500 [11.000, 29.000] - loss: 0.019 - mae: 3.266 - mean_q: 3.930 - mean_eps: 0.100 - ale.lives: 1.961\n",
      "\n",
      "Interval 246 (2450000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0218\n",
      "12 episodes - episode_reward: 19.250 [3.000, 35.000] - loss: 0.017 - mae: 3.276 - mean_q: 3.943 - mean_eps: 0.100 - ale.lives: 2.051\n",
      "\n",
      "Interval 247 (2460000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0234\n",
      "14 episodes - episode_reward: 16.786 [5.000, 29.000] - loss: 0.019 - mae: 3.324 - mean_q: 4.002 - mean_eps: 0.100 - ale.lives: 1.859\n",
      "\n",
      "Interval 248 (2470000 steps performed)\n",
      "10000/10000 [==============================] - 409s 41ms/step - reward: 0.0218\n",
      "13 episodes - episode_reward: 16.769 [4.000, 26.000] - loss: 0.019 - mae: 3.391 - mean_q: 4.081 - mean_eps: 0.100 - ale.lives: 1.934\n",
      "\n",
      "Interval 249 (2480000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0234\n",
      "11 episodes - episode_reward: 21.273 [12.000, 33.000] - loss: 0.019 - mae: 3.415 - mean_q: 4.109 - mean_eps: 0.100 - ale.lives: 2.098\n",
      "\n",
      "Interval 250 (2490000 steps performed)\n",
      "10000/10000 [==============================] - 409s 41ms/step - reward: 0.0222\n",
      "done, took 89186.631 seconds\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint\n",
    "checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'\n",
    "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=10000)]\n",
    "# Log\n",
    "log_filename = 'dqn_{}_log.json'.format(env_name)\n",
    "callbacks += [FileLogger(log_filename, interval=10000)]\n",
    "\n",
    "# Train\n",
    "dqn.fit(env, callbacks=callbacks, nb_steps=2500000, log_interval=10000, visualize=False)\n",
    "\n",
    "# Save Weights\n",
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "dqn.save_weights(weights_filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7d61f51",
    "outputId": "b7c4553b-531b-4bf1-9075-652357714f1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 25.000, steps: 966\n",
      "Episode 2: reward: 26.000, steps: 998\n",
      "Episode 3: reward: 25.000, steps: 917\n",
      "Episode 4: reward: 23.000, steps: 913\n",
      "Episode 5: reward: 21.000, steps: 876\n",
      "Episode 6: reward: 34.000, steps: 1218\n",
      "Episode 7: reward: 16.000, steps: 763\n",
      "Episode 8: reward: 30.000, steps: 1172\n",
      "Episode 9: reward: 9.000, steps: 526\n",
      "Episode 10: reward: 23.000, steps: 840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2949e6ba190>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing part to calculate the mean reward\n",
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "dqn.load_weights(weights_filename)\n",
    "dqn.test(env, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bc0910b",
    "outputId": "96d6b91e-8662-4ce0-ed2b-c004d43ae872"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculamos la media de los resultados anteriores\n",
    "np.mean(dqn.history.history['episode_reward'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_8xpRMfHtMU"
   },
   "source": [
    "Para confirmar que estos resultados eran fiables, hemos lanzado un Notebook aparte cargando los pesos y ejecutando test 5 veces consecutivas. Todos los resultados medios fueron superiores a 20 puntos, consiguiendo incluso mejorar la puntuación máxima hasta 26.4 puntos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 81002,
     "status": "ok",
     "timestamp": 1687387008189,
     "user": {
      "displayName": "Anton S",
      "userId": "11207087415458905114"
     },
     "user_tz": -120
    },
    "id": "3tuOwE5_GScK",
    "outputId": "88de0baa-d21a-45b8-f69d-452a647d5c9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 22.000, steps: 714\n",
      "Episode 2: reward: 28.000, steps: 1060\n",
      "Episode 3: reward: 35.000, steps: 1468\n",
      "Episode 4: reward: 22.000, steps: 1086\n",
      "Episode 5: reward: 27.000, steps: 1136\n",
      "Episode 6: reward: 27.000, steps: 833\n",
      "Episode 7: reward: 20.000, steps: 838\n",
      "Episode 8: reward: 32.000, steps: 1316\n",
      "Episode 9: reward: 32.000, steps: 1351\n",
      "Episode 10: reward: 19.000, steps: 680\n",
      "\n",
      "Recompensa media: 26.4\n"
     ]
    }
   ],
   "source": [
    "weights_filename = \"dqn_SpaceInvaders-v0_weights.h5f\"\n",
    "dqn.load_weights(weights_filename)\n",
    "dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(\"\\nRecompensa media:\",np.mean(dqn.history.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64843,
     "status": "ok",
     "timestamp": 1687386740760,
     "user": {
      "displayName": "Anton S",
      "userId": "11207087415458905114"
     },
     "user_tz": -120
    },
    "id": "OsRlSpkzZlZu",
    "outputId": "1b21831a-1dfb-4439-a2f9-fe02a78e7e23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 11.000, steps: 495\n",
      "Episode 2: reward: 22.000, steps: 837\n",
      "Episode 3: reward: 27.000, steps: 1069\n",
      "Episode 4: reward: 22.000, steps: 1000\n",
      "Episode 5: reward: 15.000, steps: 625\n",
      "Episode 6: reward: 25.000, steps: 970\n",
      "Episode 7: reward: 8.000, steps: 399\n",
      "Episode 8: reward: 22.000, steps: 774\n",
      "Episode 9: reward: 32.000, steps: 1436\n",
      "Episode 10: reward: 30.000, steps: 1044\n",
      "\n",
      "Recompensa media: 21.4\n"
     ]
    }
   ],
   "source": [
    "weights_filename = \"dqn_SpaceInvaders-v0_weights.h5f\"\n",
    "dqn.load_weights(weights_filename)\n",
    "dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(\"\\nRecompensa media:\",np.mean(dqn.history.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67945,
     "status": "ok",
     "timestamp": 1687386848991,
     "user": {
      "displayName": "Anton S",
      "userId": "11207087415458905114"
     },
     "user_tz": -120
    },
    "id": "TKlMihfbFuzh",
    "outputId": "70748aa3-7e7a-48f7-89ee-ccbc32fa57eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 26.000, steps: 975\n",
      "Episode 2: reward: 13.000, steps: 508\n",
      "Episode 3: reward: 33.000, steps: 1202\n",
      "Episode 4: reward: 19.000, steps: 662\n",
      "Episode 5: reward: 15.000, steps: 671\n",
      "Episode 6: reward: 6.000, steps: 352\n",
      "Episode 7: reward: 31.000, steps: 1158\n",
      "Episode 8: reward: 28.000, steps: 1153\n",
      "Episode 9: reward: 34.000, steps: 1097\n",
      "Episode 10: reward: 22.000, steps: 1012\n",
      "\n",
      "Recompensa media: 22.7\n"
     ]
    }
   ],
   "source": [
    "weights_filename = \"dqn_SpaceInvaders-v0_weights.h5f\"\n",
    "dqn.load_weights(weights_filename)\n",
    "dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(\"\\nRecompensa media:\",np.mean(dqn.history.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61887,
     "status": "ok",
     "timestamp": 1687386917141,
     "user": {
      "displayName": "Anton S",
      "userId": "11207087415458905114"
     },
     "user_tz": -120
    },
    "id": "vn-5SPBDGA6o",
    "outputId": "92ca5c3a-f9f3-450e-b4d9-a14744a769a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 21.000, steps: 704\n",
      "Episode 2: reward: 16.000, steps: 691\n",
      "Episode 3: reward: 13.000, steps: 618\n",
      "Episode 4: reward: 20.000, steps: 856\n",
      "Episode 5: reward: 27.000, steps: 963\n",
      "Episode 6: reward: 25.000, steps: 1036\n",
      "Episode 7: reward: 20.000, steps: 717\n",
      "Episode 8: reward: 29.000, steps: 1223\n",
      "Episode 9: reward: 20.000, steps: 579\n",
      "Episode 10: reward: 21.000, steps: 883\n",
      "\n",
      "Recompensa media: 21.2\n"
     ]
    }
   ],
   "source": [
    "weights_filename = \"dqn_SpaceInvaders-v0_weights.h5f\"\n",
    "dqn.load_weights(weights_filename)\n",
    "dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(\"\\nRecompensa media:\",np.mean(dqn.history.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71676,
     "status": "ok",
     "timestamp": 1687387086808,
     "user": {
      "displayName": "Anton S",
      "userId": "11207087415458905114"
     },
     "user_tz": -120
    },
    "id": "xqxu79lVGoDs",
    "outputId": "a79c6217-42b8-43c9-8b1b-f081a188ec25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 18.000, steps: 681\n",
      "Episode 2: reward: 19.000, steps: 828\n",
      "Episode 3: reward: 26.000, steps: 904\n",
      "Episode 4: reward: 27.000, steps: 1059\n",
      "Episode 5: reward: 24.000, steps: 984\n",
      "Episode 6: reward: 16.000, steps: 656\n",
      "Episode 7: reward: 22.000, steps: 750\n",
      "Episode 8: reward: 32.000, steps: 1199\n",
      "Episode 9: reward: 16.000, steps: 642\n",
      "Episode 10: reward: 20.000, steps: 776\n",
      "\n",
      "Recompensa media: 22.0\n"
     ]
    }
   ],
   "source": [
    "weights_filename = \"dqn_SpaceInvaders-v0_weights.h5f\"\n",
    "dqn.load_weights(weights_filename)\n",
    "dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(\"\\nRecompensa media:\",np.mean(dqn.history.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NAlu8b1Gb2b"
   },
   "source": [
    "3. Justificación de los parámetros seleccionados y de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ki1PErNaw2bq"
   },
   "source": [
    "Hemos probado 4 arquitecturas diferentes subiendo de complejidad. El modelo que obtuvo los mejores resultados es el que se presentó en los apartados anteriores, consiguiendo un **resultado en test de 23.2** (mejorado hasta 26.4 en pruebas sucesivas).\n",
    "\n",
    "Los hiperparámetros se fueron seleccionando en base a diferentes pruebas y observaciones, resultando en los siguientes cambios en comparación con lo visto en clase:\n",
    "- target_model_update=2000\n",
    "- train_interval=8\n",
    "- learning_rate=1e-4\n",
    "- entrenamiento durante 2.5 millones de iteraciones.\n",
    "\n",
    "El hiperparámetro \"target_model_update\" se estableció en 2000, lo que implicó actualizaciones regulares del modelo objetivo. Esta configuración permitió mantener una estimación más precisa del valor de las acciones y promovió la estabilidad del aprendizaje alineando el modelo objetivo con el modelo principal.\n",
    "\n",
    "El hiperparámetro \"train_interval\" se fijó en 8, lo que implicó actualizaciones frecuentes del modelo durante el entrenamiento. Esta configuración permitió una adaptación más ágil a los cambios del entorno y mejoró la eficiencia del proceso de entrenamiento.\n",
    "\n",
    "El hiperparámetro \"learning_rate\" se estableció en 1e-4. Al seleccionar un valor bajo, se permitió un aprendizaje más lento y gradual, evitando cambios bruscos que podrían afectar negativamente al rendimiento del agente. Esto permitió una convergencia más estable y la capacidad de aprender patrones sutiles en el entorno.\n",
    "\n",
    "Para facilitar la exploración durante el entrenamiento hemos utilizado el algoritmo LinearAnnealedPolicy. Esto nos permitió ir reduciendo el valor de epsilon (probabilidad de tomar acción aleatoria) desde el valor máximo del 100% hasta el 10%, de manera lineal y a lo largo de 1 millón de pasos. Además se estableció un 5% de probabilidad de acción aleatoria durante el test.\n",
    "\n",
    "Se realizó el entrenamiento durante 2.5 millones de iteraciones, lo que proporcionó al modelo suficiente tiempo para adaptarse y aprender de manera continua sin interrupciones. Este modelo fue el único que hemos conseguido entrenar durante tantos pasos sin ninguna interrupción, lo que en nuestra opinión fue un factor importante para conseguir los mejores resultados.\n",
    "\n",
    "A continuación describimos las dificultades con las que nos hemos encontrado durante la realización del presente proyecto, así como las diferentes arquitecturas probadas y su justificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxM2i7SNz2WO"
   },
   "source": [
    "**DIFICULTADES**\n",
    "\n",
    "Los primeros entrenamientos fueron realizados en Google Colab porque era más sencillo y rápido de configurar y porque había integrantes del grupo cuyas máquinas no admitían configuración en el entorno local. En Colab, la principal parte de los entrenamientos fue realizada en entornos CPU, aunque vimos que modelos pesados se entrenaban considerablemente más rápido en GPU. El problema principal fueron interrupciones frecuentes durante entrenamiento en Colab debido a diferentes causas: captcha \"No soy robot\", desconexión temprana por el límite de uso de GPU, u otras razones que no se podían identificar a la mañana siguiente. El problema persistió incluso tras adquirir unidades de computación a través de servicio de pago.\n",
    "\n",
    "Hemos observado que en todos los casos cuando se presentaban interrupciones, los resultados tanto durante el entrenamiento como luego en test iban mejorando hasta un cierto momento, tras el cual empezaban a empeorar hasta el punto que un modelo entrenado durante 2 millones de pasos, pero con interrupciones, daba peores resultados que un modelo sin entrenar.\n",
    "\n",
    "Por supuesto en todo momento hemos utilizado los checkpoints, y hemos automatizado el re-calculo de los parametros de epsilon, cuyo código se presenta en el anexo. Hemos pensado que se podía provocar por el hecho de que cada nuevo entrenamiento comienza con la memoria vacía, y para solucionarlo hemos probado grabar la memoria en un fichero con la libreria pickle. No obstante no pudimos aplicar este metodo, debido a que pickle guarda sin comprimir, por lo que el fichero se vuelve pesado rápidamente, saturando la RAM del Colab.\n",
    "\n",
    "Finalmente hemos pasado a entorno local, donde tras solucionar también una serie de dificultades, hemos conseguido entrenamientos continuos que finalmente presentaron los mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ijViVwC0AfD"
   },
   "source": [
    "**ARQUITECTURAS PROBADAS**\n",
    "\n",
    "Hemos comenzado nuestras pruebas con la implementación de la arquitectura presentada en el artículo Playing Atari with Deep Reinforcement Learning (https://arxiv.org/abs/1312.5602). Desafortunadamente trás 2 millones de steps los resultados en test apenas superaban los de un modelo sin entrenar. Creemos que se debe a las interrupciones en el entrenamiento, quizás un entrenamiento continuo daría mejores resultados. Cabe mencionar que los autores del artículo entrenaron sus modelos durante 10 millones de pasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1260,
     "status": "ok",
     "timestamp": 1686655978688,
     "user": {
      "displayName": "Anton S",
      "userId": "11207087415458905114"
     },
     "user_tz": -120
    },
    "id": "NyHCHW63DdJf",
    "outputId": "1ec25586-b115-4b4b-b9ab-c67909aa6e9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " permute (Permute)           (None, 84, 84, 4)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 20, 20, 16)        4112      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 20, 20, 16)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 9, 9, 32)          8224      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 9, 9, 32)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2592)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               663808    \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 1542      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 6)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 677,686\n",
      "Trainable params: 677,686\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.add(Convolution2D(16, (8, 8), strides=(4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(32, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--eUPXqfHDs7"
   },
   "source": [
    "En vistas de no conseguir los resultados necesarios, fuimos subiendo la complejidad de nuestro modelo, añadiendo capas convolucionales/densas y subiendo número de filtros/neuronas. En comparación con la arquitectura del articulo, la siguiente red presenta una capa convolucional adicional, subiendo además tanto el número de filtros como el número de neuronas. Con esta red tras 1 millon de steps hemos conseguido un resultado en test de 20.9 , aunque los resultados empeoraron al continuar entrenamiento (con interrupciones) hasta 2 millones de pasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1686655588904,
     "user": {
      "displayName": "Anton S",
      "userId": "11207087415458905114"
     },
     "user_tz": -120
    },
    "id": "J3yIsh9pHnf0",
    "outputId": "cc1cd3a1-6048-405e-e01e-d42c2c39f399"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " permute (Permute)           (None, 84, 84, 4)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 20, 20, 32)        8224      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 20, 20, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 9, 9, 64)          32832     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 9, 9, 64)          0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1606144   \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 3078      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 6)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,687,206\n",
      "Trainable params: 1,687,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62351,
     "status": "ok",
     "timestamp": 1686518713348,
     "user": {
      "displayName": "Anton S",
      "userId": "11207087415458905114"
     },
     "user_tz": -120
    },
    "id": "zoPu0mOeWCVQ",
    "outputId": "2d2e05fb-94f8-4b59-ce82-157789ddf66e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 17.000, steps: 786\n",
      "Episode 2: reward: 16.000, steps: 659\n",
      "Episode 3: reward: 27.000, steps: 942\n",
      "Episode 4: reward: 19.000, steps: 1089\n",
      "Episode 5: reward: 23.000, steps: 1117\n",
      "Episode 6: reward: 18.000, steps: 865\n",
      "Episode 7: reward: 24.000, steps: 892\n",
      "Episode 8: reward: 18.000, steps: 795\n",
      "Episode 9: reward: 20.000, steps: 824\n",
      "Episode 10: reward: 27.000, steps: 1226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_filename = 'dqn_{}_weights_1080000.h5f'.format(env_name)\n",
    "dqn.load_weights(weights_filename)\n",
    "dqn.test(env, nb_episodes=10, visualize=False)\n",
    "np.mean(dqn.history.history['episode_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58638,
     "status": "ok",
     "timestamp": 1686518843773,
     "user": {
      "displayName": "Anton S",
      "userId": "11207087415458905114"
     },
     "user_tz": -120
    },
    "id": "zGAU-8GfWktc",
    "outputId": "e3490ca5-e710-44f9-e5c4-2197d1d3352c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 24.000, steps: 1036\n",
      "Episode 2: reward: 16.000, steps: 745\n",
      "Episode 3: reward: 21.000, steps: 805\n",
      "Episode 4: reward: 20.000, steps: 1124\n",
      "Episode 5: reward: 39.000, steps: 1508\n",
      "Episode 6: reward: 12.000, steps: 673\n",
      "Episode 7: reward: 11.000, steps: 521\n",
      "Episode 8: reward: 23.000, steps: 949\n",
      "Episode 9: reward: 16.000, steps: 863\n",
      "Episode 10: reward: 27.000, steps: 1035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_filename = 'dqn_{}_weights_1080000.h5f'.format(env_name)\n",
    "dqn.load_weights(weights_filename)\n",
    "dqn.test(env, nb_episodes=10, visualize=False)\n",
    "np.mean(dqn.history.history['episode_reward'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEGTgval0izi"
   },
   "source": [
    "A continuación hemos añadido una capa densa adicional con 256 neuronas. Este es el modelo que hemos presentado en la sección del Desarrollo del presente trabajo. Tras un entrenamiento continuo durante 2.5 millones de iteraciones, esta arquitectura fue la que dio los mejores resultados en test de 23.2 puntos de recompensa media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1687296295682,
     "user": {
      "displayName": "Anton S",
      "userId": "11207087415458905114"
     },
     "user_tz": -120
    },
    "id": "2mOSp5lW05nS",
    "outputId": "582f1020-fa5e-48bd-84a5-f4fac1b5207c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " permute (Permute)           (None, 84, 84, 4)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 20, 20, 32)        8224      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 9, 9, 64)          32832     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1606144   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,816,998\n",
      "Trainable params: 1,816,998\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4), activation='relu'))\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(nb_actions, activation='linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrzz2IUN07p7"
   },
   "source": [
    "La arquitectura final que hemos probado fue la más pesada de todas, incrementando el número de neuronas de las capas densas del modelo anterior a 1024 y 512 respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1686573625056,
     "user": {
      "displayName": "Anton S",
      "userId": "11207087415458905114"
     },
     "user_tz": -120
    },
    "id": "4JDo1oGRlqWy",
    "outputId": "6d02c848-5f3c-4434-ac0d-2d343123e396"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " permute (Permute)           (None, 84, 84, 4)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 20, 20, 32)        8224      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 20, 20, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 9, 9, 64)          32832     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 9, 9, 64)          0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              6423552   \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 3078      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 6)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,066,342\n",
      "Trainable params: 7,066,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(128, (3, 3), strides=(1, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pq__oZcWHyVf"
   },
   "source": [
    "El entrenamiento de la arquitectura mostrada anteriormente se tuvo que realizar de forma fraccionada debido a interrupciones del kernel, consto de 4 entrenamientos, cada vez se interrupía el entrenamiento iniciabamos otro de forma sucesiva utilizando los últimos pesos actualizados del anterior,  pese a las interrupciones mencionadas, cabe descatacar que se entreno en unas 16 horas aproximadamente con un nb_step=2000000 (tiempo bastante razonable para la cantidad de parámetros entrenables disponibles), aun así los resultados que obtuvimos fueron inferiores a los de la arquitectura del modelo que presentamos como candidata a resolver el desafío.\n",
    "\n",
    "A continuación se muestra la evaluacion en test para 10 episodios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 80306,
     "status": "ok",
     "timestamp": 1686575043821,
     "user": {
      "displayName": "Ana Rodriguez Dominguez",
      "userId": "17093722423994411636"
     },
     "user_tz": -120
    },
    "id": "6ASaoM-s4itL",
    "outputId": "afaf90ce-83a9-4573-ed60-3b3c00a92e3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 15.000, steps: 627\n",
      "Episode 2: reward: 21.000, steps: 757\n",
      "Episode 3: reward: 22.000, steps: 849\n",
      "Episode 4: reward: 22.000, steps: 875\n",
      "Episode 5: reward: 10.000, steps: 636\n",
      "Episode 6: reward: 28.000, steps: 1475\n",
      "Episode 7: reward: 33.000, steps: 1482\n",
      "Episode 8: reward: 22.000, steps: 888\n",
      "Episode 9: reward: 16.000, steps: 627\n",
      "Episode 10: reward: 24.000, steps: 1019\n",
      "\n",
      "Recompensa media: 21.3\n"
     ]
    }
   ],
   "source": [
    "# Cargar los últimos pesos\n",
    "weights_filename = \"./Checkpoints/\"+ 'dqn_{}_weights_{}.h5f'.format(env_name, last_checkpoint)\n",
    "dqn.load_weights(weights_filename)\n",
    "dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(\"\\nRecompensa media:\",np.mean(dqn.history.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UF_HHhPomGLe"
   },
   "source": [
    "Observamos que las recompensas son bastante inestables en función del episodio que se evalúa, obtenemos recompensas muy buenas de hasta 33 y recompensas de 10, muy por debajo del objetivo marcado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyDGnFzx1Off"
   },
   "source": [
    "**ANEXO**\n",
    "\n",
    "Aquí se presenta el código que fuimos implementando para intentar solucionar el problema de las interrupciones de entrenamiento. Tal y como explicamos anteriormente, los mejores resultados conseguidos finalmente vienen de los entrenamientos sin interrupciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "denW8gQvmEDp"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_memory(memory, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(memory, file)\n",
    "\n",
    "def save_policy_params(attr, value_max, value_min, value_test, nb_steps, filename):\n",
    "    policy_params = {\n",
    "    'attr': attr,\n",
    "    'value_max': value_max,\n",
    "    'value_min': value_min,\n",
    "    'value_test': value_test,\n",
    "    'nb_steps': nb_steps\n",
    "    }\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(policy_params, file)\n",
    "\n",
    "def save_training_info(nb_steps, filename):\n",
    "    training_info = {\n",
    "    'nb_steps': nb_steps\n",
    "    }\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(training_info, f)\n",
    "\n",
    "def load_memory(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        memory = pickle.load(f)\n",
    "    return memory\n",
    "\n",
    "def load_policy_params(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        policy_params = pickle.load(f)\n",
    "    return policy_params\n",
    "\n",
    "def load_training_info(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        training_info = pickle.load(f)\n",
    "    return training_info\n",
    "\n",
    "\n",
    "# Variable PRIMER_ENTRENAMIENTO indica:\n",
    "    #True -> se va a hacer el entrenamiento desde cero.\n",
    "    #False -> se va a retomar el entrenamiento desde la info almacenada.\n",
    "try:\n",
    "    archivos = []\n",
    "    for archivo in os.listdir(\"./Checkpoints/Entrenamiento CNN_memoria_2/Weights/\"):\n",
    "        nombre, extension = os.path.splitext(archivo)\n",
    "        if 'weights_' in nombre:\n",
    "            num_pasos = nombre.split('_')[3].split('.')[0]\n",
    "            if '+' in num_pasos:\n",
    "                num_pasos_act, num_pasos_ant  = num_pasos.split('+')\n",
    "                num_pasos = int(num_pasos_ant) + int(num_pasos_act)\n",
    "                archivos.append(num_pasos)\n",
    "    last_checkpoint = max(archivos)\n",
    "    PRIMER_ENTRENAMIENTO = False\n",
    "\n",
    "except:\n",
    "    last_checkpoint = 0\n",
    "    PRIMER_ENTRENAMIENTO = True\n",
    "\n",
    "# Directorio para guardar los pesos y registros\n",
    "checkpoint_dir = \"./Checkpoints/Entrenamiento CNN_memoria_2/Weights/\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "if not PRIMER_ENTRENAMIENTO:\n",
    "    checkpoint_weights_filename = checkpoint_dir + 'dqn_' + env_name + '_weights_{step}' + '+{}.h5f'.format(last_checkpoint)\n",
    "else:\n",
    "    checkpoint_weights_filename = checkpoint_dir + 'dqn_' + env_name + '_weights_{step}.h5f'\n",
    "print(checkpoint_weights_filename)\n",
    "weights_filename = checkpoint_dir + 'dqn_{}_weights.h5f'.format(env_name)\n",
    "log_filename = checkpoint_dir + 'dqn_{}_log.json'.format(env_name)\n",
    "\n",
    "#Directorio para guardar la memoria\n",
    "memory_dir = \"./Checkpoints/Entrenamiento CNN_memoria_2/Memory/\"\n",
    "os.makedirs(memory_dir, exist_ok=True)\n",
    "memory_filename = memory_dir + 'dqn_{}_memory.pkl'.format(env_name)\n",
    "\n",
    "# Directorio para guardar la policy\n",
    "policy_dir = \"./Checkpoints/Entrenamiento CNN_memoria_2/Policy/\"\n",
    "os.makedirs(policy_dir, exist_ok=True)\n",
    "policy_filename = policy_dir + 'dqn_{}_policy.pkl'.format(env_name)\n",
    "\n",
    "#Directorio para guardar la info de training\n",
    "#training_info_dir = \"./Checkpoints/Entrenamiento CNN_memoria/Training_info/\"\n",
    "#os.makedirs(training_info_dir, exist_ok=True)\n",
    "#training_info_filename = training_info_dir + 'dqn_{}_training_info.pkl'.format(env_name)\n",
    "\n",
    "processor = AtariProcessor()\n",
    "\n",
    "if PRIMER_ENTRENAMIENTO:\n",
    "    warmup = 50000\n",
    "\n",
    "    memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
    "                                      value_max=1., value_min=.1, value_test=.2,\n",
    "                                      nb_steps=1000000)\n",
    "\n",
    "    dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy,\n",
    "                    memory=memory, processor=processor,\n",
    "                    nb_steps_warmup=warmup, gamma=.99,\n",
    "                    target_model_update=2000,\n",
    "                    train_interval=8)\n",
    "    dqn.compile(Adam(learning_rate=1e-4), metrics=['mae'])\n",
    "\n",
    "else:\n",
    "    warmup = 0\n",
    "\n",
    "    memory = load_memory(memory_filename)\n",
    "\n",
    "    #training_info = load_training_info(training_info_filename)\n",
    "\n",
    "    policy_params = load_policy_params(policy_filename)\n",
    "\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), **policy_params)\n",
    "\n",
    "    dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy,\n",
    "                    memory=memory, processor=processor,\n",
    "                    nb_steps_warmup=warmup, gamma=.99,\n",
    "                    target_model_update=2000,\n",
    "                    train_interval=8)\n",
    "    dqn.compile(Adam(learning_rate=1e-4), metrics=['mae'])\n",
    "\n",
    "    # Cargar los valores y pesos\n",
    "    dqn.step = training_info['nb_steps']\n",
    "    dqn.load_weights(weights_filename)\n",
    "\n",
    "\n",
    "# Checkpoint\n",
    "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=10000)]\n",
    "# Log\n",
    "callbacks += [FileLogger(log_filename, interval=10000)]\n",
    "\n",
    "total_steps = 2500000\n",
    "\n",
    "# Train\n",
    "if PRIMER_ENTRENAMIENTO:\n",
    "    dqn.fit(env, callbacks=callbacks, nb_steps=total_steps, log_interval=10000, visualize=False)\n",
    "\n",
    "else:\n",
    "    remaining_steps = total_steps - last_checkpoint\n",
    "    dqn.fit(env, callbacks=callbacks, nb_steps=remaining_steps, log_interval=10000, visualize=False)\n",
    "\n",
    "# Save Weights\n",
    "dqn.save_weights(weights_filename, overwrite=True)\n",
    "# Save memory\n",
    "save_memory(dqn.memory, memory_filename)\n",
    "# Save policy\n",
    "save_policy_params(dqn.policy.attr, dqn.policy.value_max, dqn.policy.value_min, dqn.policy.value_test, dqn.policy.nb_steps, policy_filename)\n",
    "#Save training info\n",
    "save_training_info(dqn.step, training_info_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANFQiicXK3sO"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
